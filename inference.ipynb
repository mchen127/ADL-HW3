{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1cc7ae67a848461f81347059d71893cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80aa12e991a04acfa7743f8b9b1e62ff",
              "IPY_MODEL_7a6dd8e82e5749c286ef5618579202ac",
              "IPY_MODEL_fc60f9746ad44c33aabe64cbc93819ae"
            ],
            "layout": "IPY_MODEL_b11e6cb1b35347679ee73be3c9db980e"
          }
        },
        "80aa12e991a04acfa7743f8b9b1e62ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a0f66edf67a4cbd9e8d90bcde5496dc",
            "placeholder": "​",
            "style": "IPY_MODEL_3ea67e716f1a4c8693ebd0f8929d1983",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7a6dd8e82e5749c286ef5618579202ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb215205dceb4f998ac0308f9f6f7afe",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcdc998d51494a00ace8fb9acba96d1b",
            "value": 3
          }
        },
        "fc60f9746ad44c33aabe64cbc93819ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_054fcfecb1424f5e8aaf30a7e4b1668c",
            "placeholder": "​",
            "style": "IPY_MODEL_f417837d13d248df8a6ba2ca219593d2",
            "value": " 3/3 [00:03&lt;00:00,  1.08s/it]"
          }
        },
        "b11e6cb1b35347679ee73be3c9db980e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a0f66edf67a4cbd9e8d90bcde5496dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea67e716f1a4c8693ebd0f8929d1983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb215205dceb4f998ac0308f9f6f7afe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcdc998d51494a00ace8fb9acba96d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "054fcfecb1424f5e8aaf30a7e4b1668c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f417837d13d248df8a6ba2ca219593d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.1 transformers==4.45.1 bitsandbytes==0.44.1 peft==0.13.0 datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "g3RmFC8Eb-Xk",
        "outputId": "cb5e0390-4725-4b47-a109-ff3688c2cb98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n",
            "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting transformers==4.45.1\n",
            "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.44.1\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting peft==0.13.0\n",
            "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.1)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
            "  Downloading tokenizers-0.20.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.13.0) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.0) (0.34.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1) (12.6.77)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch==2.4.1)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusolver-cu12, nvidia-cudnn-cu12, multiprocess, torch, tokenizers, transformers, bitsandbytes, peft, datasets\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.13.2\n",
            "    Uninstalling peft-0.13.2:\n",
            "      Successfully uninstalled peft-0.13.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.4.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.44.1 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 peft-0.13.0 tokenizers-0.20.2 torch-2.4.1 transformers-4.45.1 triton-3.0.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "peft",
                  "torch",
                  "torchgen",
                  "transformers"
                ]
              },
              "id": "feb6705f82dc4dea85708c9a76c34771"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6GFg_shCaPtf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, LoraConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import gdown\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model folder URL\n",
        "model_folder_url = \"https://drive.google.com/drive/folders/11qu4SrHmHoM2SBLQwoodbcd88ZziI3Vp?usp=sharing\"\n",
        "\n",
        "# Convert the folder link to a format gdown can recognize\n",
        "gdown.download_folder(model_folder_url, quiet=False, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMjWuiYraWGT",
        "outputId": "46f1070e-2f9d-4815-9421-af5758f109e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 125-XJNMKToeEjchCWqNTslfGJK9Wtmtg adapter_config.json\n",
            "Processing file 12Ex9G4KUqa2fE4BJv18V_mPPdPZwNmzN adapter_model.safetensors\n",
            "Processing file 12GyOX02I3mtDtRQ334DW07YD1HLOxlTN README.md\n",
            "Processing file 11x4tYNAYSrVwVTDvNJ_hkLaWsoWVZf9- special_tokens_map.json\n",
            "Processing file 121U_sOThuoOaaBnhWcMA3yEtk_cJyaj8 tokenizer_config.json\n",
            "Processing file 11vCYUzoxN7btV9zpm7YIoOk60gqAm6cL tokenizer.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=125-XJNMKToeEjchCWqNTslfGJK9Wtmtg\n",
            "To: /content/epoch_1/adapter_config.json\n",
            "100%|██████████| 664/664 [00:00<00:00, 1.86MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12Ex9G4KUqa2fE4BJv18V_mPPdPZwNmzN\n",
            "To: /content/epoch_1/adapter_model.safetensors\n",
            "100%|██████████| 6.40M/6.40M [00:00<00:00, 17.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12GyOX02I3mtDtRQ334DW07YD1HLOxlTN\n",
            "To: /content/epoch_1/README.md\n",
            "100%|██████████| 5.11k/5.11k [00:00<00:00, 14.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11x4tYNAYSrVwVTDvNJ_hkLaWsoWVZf9-\n",
            "To: /content/epoch_1/special_tokens_map.json\n",
            "100%|██████████| 636/636 [00:00<00:00, 713kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=121U_sOThuoOaaBnhWcMA3yEtk_cJyaj8\n",
            "To: /content/epoch_1/tokenizer_config.json\n",
            "100%|██████████| 47.1k/47.1k [00:00<00:00, 37.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11vCYUzoxN7btV9zpm7YIoOk60gqAm6cL\n",
            "To: /content/epoch_1/tokenizer.json\n",
            "100%|██████████| 34.4M/34.4M [00:00<00:00, 35.5MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/epoch_1/adapter_config.json',\n",
              " '/content/epoch_1/adapter_model.safetensors',\n",
              " '/content/epoch_1/README.md',\n",
              " '/content/epoch_1/special_tokens_map.json',\n",
              " '/content/epoch_1/tokenizer_config.json',\n",
              " '/content/epoch_1/tokenizer.json']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive file ID from your link\n",
        "file_id = \"18FRjHAL_N90uKv3nsKE0ChBf8fY1qFhE\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Specify the output file path\n",
        "output_path = \"private_test.json\"\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "print(f\"File downloaded and saved as '{output_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmPgjUDtbHlA",
        "outputId": "f7f1917d-000b-4c51-fadc-81377f4bdbd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18FRjHAL_N90uKv3nsKE0ChBf8fY1qFhE\n",
            "To: /content/private_test.json\n",
            "100%|██████████| 48.4k/48.4k [00:00<00:00, 46.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as 'private_test.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset file URL\n",
        "dataset_url = \"https://drive.google.com/file/d/1mQInr4_t1IZI5ZQFsTyg-Ln9LLudFBIj\"\n",
        "\n",
        "# Download the dataset file\n",
        "gdown.download(dataset_url, output=\"public_test.json\", quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "RYrT3kvLem-S",
        "outputId": "09b925e3-94de-4808-95bb-2f9caa9d5438"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/1mQInr4_t1IZI5ZQFsTyg-Ln9LLudFBIj\n",
            "To: /content/private_test.json\n",
            "92.4kB [00:00, 2.57MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'private_test.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"epoch_1\""
      ],
      "metadata": {
        "id": "ty3xVn_FbdE_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model from the saved checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_name = \"zake7749/gemma-2-2b-it-chinese-kyara-dpo\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1cc7ae67a848461f81347059d71893cf",
            "80aa12e991a04acfa7743f8b9b1e62ff",
            "7a6dd8e82e5749c286ef5618579202ac",
            "fc60f9746ad44c33aabe64cbc93819ae",
            "b11e6cb1b35347679ee73be3c9db980e",
            "2a0f66edf67a4cbd9e8d90bcde5496dc",
            "3ea67e716f1a4c8693ebd0f8929d1983",
            "cb215205dceb4f998ac0308f9f6f7afe",
            "dcdc998d51494a00ace8fb9acba96d1b",
            "054fcfecb1424f5e8aaf30a7e4b1668c",
            "f417837d13d248df8a6ba2ca219593d2"
          ]
        },
        "id": "fsqU6YJZaYj8",
        "outputId": "657e07b8-0cfe-4913-ffd5-c0baaa25cd4e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cc7ae67a848461f81347059d71893cf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt(instruction: str) -> str:\n",
        "    '''Format the instruction as a prompt for LLM.'''\n",
        "    return f\"請你扮演一個人工智慧國文助理，\\\n",
        "幫助用戶在白話文和文言文之間轉換，\\\n",
        "USER: {instruction} \\\n",
        "ASSISTANT:\\\n",
        "\""
      ],
      "metadata": {
        "id": "p-fpgv3XgB84"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "JIqObPKntdOE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n41eIoHktksd",
        "outputId": "ddc3d1f7-75be-4756-c57a-c9e4d2794782"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma2ForCausalLM(\n",
              "      (model): Gemma2Model(\n",
              "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-25): 26 x Gemma2DecoderLayer(\n",
              "            (self_attn): Gemma2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
              "              (rotary_emb): Gemma2RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Gemma2MLP(\n",
              "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "private_test_path = \"private_test.json\""
      ],
      "metadata": {
        "id": "kNIvht5WpkN6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "with open(private_test_path, \"r\") as f:\n",
        "    private_test_data = json.load(f)"
      ],
      "metadata": {
        "id": "_wyzFrIEoOx_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract instructions from the dataset for inference\n",
        "instructions = [get_prompt(item[\"instruction\"]) for item in private_test_data]"
      ],
      "metadata": {
        "id": "qOvwvq3ltVVb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for i, instruction in enumerate(tqdm(instructions, desc=\"Processing Instructions\")):\n",
        "    # Prepare input tensor for model\n",
        "    inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output using the model\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,  # Adjust max_length if needed\n",
        "            num_beams=15,     # Beam search can improve results\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the part after \"ASSISTANT:\" in the generated output\n",
        "    assistant_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "    # Append the result to predictions list\n",
        "    predictions.append({\n",
        "        \"id\": private_test_data[i][\"id\"],\n",
        "        \"output\": assistant_text\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f9Pt1esmldn",
        "outputId": "d90a7618-a908-4426-da93-c3cf2d20a064"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Instructions: 100%|██████████| 250/250 [15:33<00:00,  3.73s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to a JSON file\n",
        "with open(\"prediction.json\", \"w\") as f:\n",
        "    json.dump(predictions, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Inference completed and predictions saved to prediction.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUzlMYZNs33F",
        "outputId": "6713c9eb-a7d8-4f56-d146-c906dad0fa6d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed and predictions saved to prediction.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqtKa2wsuzsD",
        "outputId": "f73e8ecd-b779-4a32-d37b-d54bc477f0df"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma2ForCausalLM(\n",
              "  (model): Gemma2Model(\n",
              "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-25): 26 x Gemma2DecoderLayer(\n",
              "        (self_attn): Gemma2Attention(\n",
              "          (q_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2304, out_features=8, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
              "          (v_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2304, out_features=8, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
              "          (rotary_emb): Gemma2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Gemma2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
              "  (_cache): HybridCache()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison"
      ],
      "metadata": {
        "id": "m_HNmXDf7Mcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Select 10 random samples from the dataset\n",
        "random.seed(42)  # For reproducibility\n",
        "sample_data = random.sample(private_test_data, 10)"
      ],
      "metadata": {
        "id": "cJLYTNgk1vFR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eq0tNxsARIj",
        "outputId": "ac4c1742-190f-4a89-e04e-7bc1042681b7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'c795e8da-6602-441b-9632-a6eb4d5e429f',\n",
              "  'instruction': '建武元年，拜為禦史中丞。\\n把這句話翻譯成文言文：'},\n",
              " {'id': '0bd01970-ca46-4029-ab5a-c31b68d550d6',\n",
              "  'instruction': '文言文翻譯：\\n上嘉納之。'},\n",
              " {'id': '7efea98b-646a-4bd8-b85c-0118d3493506',\n",
              "  'instruction': '翻譯成文言文：\\n親自耕作的原因，就是錶示重視祭祀的供品。\\n答案：'},\n",
              " {'id': 'bdb5da27-91b9-428d-ba77-921a53e174f4',\n",
              "  'instruction': '翻譯成現代文：\\n四十五年，五大夫賁攻韓，取十城。'},\n",
              " {'id': '7cfccb6e-ab89-4ae6-b38a-065ed249b12d',\n",
              "  'instruction': '陳後主說： 那就把毛喜安置在一個小郡中，不許他再在朝廷參預政事。\\n這句話在中國古代怎麼說：'},\n",
              " {'id': '134dca67-ff20-4582-a6aa-7610a0c00822',\n",
              "  'instruction': '此一難忍也。\\n翻譯成白話文：'},\n",
              " {'id': '1c494ae3-1df0-4ce8-ae4f-3857864dec25',\n",
              "  'instruction': '到崇政殿受冊就夠瞭！ 元宵節燈宴，太後的母親應當入宮，太後製止說 ：夫人登樓，皇上一定增加禮儀，這樣由於我的原因而超越典章製度，於心很不安啊！ 隻令賜給她燈燭，於是每年以此為常例。\\n這句話在古代怎麼說：'},\n",
              " {'id': '2c0caaf9-904a-425f-b516-de4dd438d66c',\n",
              "  'instruction': '以己之心為根據來要求彆人，把自己的好惡也用到彆人身上。\\n幫我把這句話翻譯成文言文'},\n",
              " {'id': '364fee8a-45bc-4f58-92d9-cafc8085fe09',\n",
              "  'instruction': '人或鑿崖取之，即風雷為變。\\n翻譯成白話文：'},\n",
              " {'id': '91650fd4-6b91-474a-b301-9e0187d4f9a3',\n",
              "  'instruction': '翻譯成文言文：\\n她高壽多少？\\n答案：'}]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LORA"
      ],
      "metadata": {
        "id": "9N4W0fxM7gss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract instructions from the dataset for inference\n",
        "instructions = [get_prompt(item[\"instruction\"]) for item in sample_data]"
      ],
      "metadata": {
        "id": "ym2AUBpb7N76"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_predictions = []\n",
        "\n",
        "for i, instruction in enumerate(tqdm(instructions, desc=\"Processing Instructions\")):\n",
        "    # Prepare input tensor for model\n",
        "    inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output using the model\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,  # Adjust max_length if needed\n",
        "            num_beams=15,     # Beam search can improve results\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the part after \"ASSISTANT:\" in the generated output\n",
        "    assistant_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "    # Append the result to predictions list\n",
        "    lora_predictions.append({\n",
        "        \"id\": private_test_data[i][\"id\"],\n",
        "        \"output\": assistant_text\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQP-WjNc7RgC",
        "outputId": "3f55f41d-73c3-4336-8617-e415e826cad6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Instructions: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to a JSON file\n",
        "with open(\"lora_prediction.json\", \"w\") as f:\n",
        "    json.dump(lora_predictions, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Inference completed and predictions saved to lora_prediction.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR0MJKGp7o-o",
        "outputId": "6f6543bc-0efa-49d6-c6a9-bdffb8cb2201"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed and predictions saved to lora_prediction.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero shot"
      ],
      "metadata": {
        "id": "yyIWN0sCvPq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_prompt(instruction: str) -> str:\n",
        "    return f\"\\\n",
        "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換\\\n",
        "你被給予的指示中會有明確要求要轉換成文言文還是白話文\\\n",
        "如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案\\\n",
        "如果這個要求在句尾，請直接回答：文言文答案/白話文答案\\\n",
        "以下是你被給予的指示：\\\n",
        "USER:{instruction} \\\n",
        "ASSISTANT:\\\n",
        "\""
      ],
      "metadata": {
        "id": "FpuKgEXm2Ez5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(zero_shot_prompt(\"djd\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc_dIhTT4zT4",
        "outputId": "c925a114-8cbb-4026-9136-b32d12543daa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:djd ASSISTANT:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_instructions = [zero_shot_prompt(item[\"instruction\"]) for item in sample_data]"
      ],
      "metadata": {
        "id": "i6tTy7sjv7wf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_instructions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "tK6CH7Sq6hGa",
        "outputId": "1fc4f668-36fb-4af0-ed46-47c9515fb0b0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:建武元年，拜為禦史中丞。\\n把這句話翻譯成文言文： ASSISTANT:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_predictions = []\n",
        "\n",
        "for i, instruction in enumerate(tqdm(zero_shot_instructions, desc=\"Processing Instructions\")):\n",
        "    # Prepare input tensor for model\n",
        "    inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output using the model\n",
        "    with torch.no_grad():\n",
        "        generated_ids = base_model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,  # Adjust max_length if needed\n",
        "            num_beams=15,     # Beam search can improve results\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(generated_text)\n",
        "\n",
        "    # Extract only the part after \"ASSISTANT:\" in the generated output\n",
        "    assistant_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "    print(assistant_text)\n",
        "    # Append the result to predictions list\n",
        "    zero_shot_predictions.append({\n",
        "        \"id\": private_test_data[i][\"id\"],\n",
        "        \"output\": assistant_text\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-HGrhwuvNoD",
        "outputId": "13b0860c-5543-4d0f-8c25-1639969c4f19"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Instructions:  10%|█         | 1/10 [00:01<00:14,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:建武元年，拜為禦史中丞。\n",
            "把這句話翻譯成文言文： ASSISTANT:建武元年，拜禦史中丞。\n",
            "建武元年，拜禦史中丞。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  20%|██        | 2/10 [00:02<00:10,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:文言文翻譯：\n",
            "上嘉納之。 ASSISTANT:答案：上嘉之。\n",
            "答案：上嘉之。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  30%|███       | 3/10 [00:04<00:10,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:翻譯成文言文：\n",
            "親自耕作的原因，就是錶示重視祭祀的供品。\n",
            "答案： ASSISTANT:親自耕作，以示重視祭祀。\n",
            "親自耕作，以示重視祭祀。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  40%|████      | 4/10 [00:07<00:11,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:翻譯成現代文：\n",
            "四十五年，五大夫賁攻韓，取十城。 ASSISTANT:四十五年，五大夫賁攻韓，取十城。\n",
            "四十五年，五大夫賁攻韓，取十城。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  50%|█████     | 5/10 [00:10<00:11,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:陳後主說： 那就把毛喜安置在一個小郡中，不許他再在朝廷參預政事。\n",
            "這句話在中國古代怎麼說： ASSISTANT:陳後主曰： 將毛喜安置於小郡，不許參政。\n",
            "陳後主曰： 將毛喜安置於小郡，不許參政。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  60%|██████    | 6/10 [00:11<00:07,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:此一難忍也。\n",
            "翻譯成白話文： ASSISTANT:難以忍耐。\n",
            "難以忍耐。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  70%|███████   | 7/10 [00:21<00:13,  4.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:到崇政殿受冊就夠瞭！ 元宵節燈宴，太後的母親應當入宮，太後製止說 ：夫人登樓，皇上一定增加禮儀，這樣由於我的原因而超越典章製度，於心很不安啊！ 隻令賜給她燈燭，於是每年以此為常例。\n",
            "這句話在古代怎麼說： ASSISTANT:到崇政殿受冊就足矣！元宵節燈宴，太後母親應入宮，太後製止說：夫人登樓，皇上必增加禮儀，故因我的原因而超越典章製度，於心不安！ 隻令賜燈燭，每年以此為常例。\n",
            "到崇政殿受冊就足矣！元宵節燈宴，太後母親應入宮，太後製止說：夫人登樓，皇上必增加禮儀，故因我的原因而超越典章製度，於心不安！ 隻令賜燈燭，每年以此為常例。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  80%|████████  | 8/10 [00:24<00:08,  4.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:以己之心為根據來要求彆人，把自己的好惡也用到彆人身上。\n",
            "幫我把這句話翻譯成文言文 ASSISTANT:以己之心為根據，要求彆人，把自己的好惡也用於彆人。\n",
            "以己之心為根據，要求彆人，把自己的好惡也用於彆人。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  90%|█████████ | 9/10 [00:26<00:03,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:人或鑿崖取之，即風雷為變。\n",
            "翻譯成白話文： ASSISTANT:人或鑿崖取之，即風雷為變。\n",
            "人或鑿崖取之，即風雷為變。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Instructions: 100%|██████████| 10/10 [00:27<00:00,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是你被給予的指示：USER:翻譯成文言文：\n",
            "她高壽多少？\n",
            "答案： ASSISTANT:高壽多少？\n",
            "高壽多少？\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to a JSON file\n",
        "with open(\"zero_shot_prediction.json\", \"w\") as f:\n",
        "    json.dump(zero_shot_predictions, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Inference completed and predictions saved to zero_shot_prediction.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlWJ99rpvl4w",
        "outputId": "66134d48-c95b-43af-a059-fb3e766116bd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed and predictions saved to zero_shot_prediction.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot"
      ],
      "metadata": {
        "id": "UZQbnwyuwV09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def few_shot_prompt(instruction: str) -> str:\n",
        "    return f\"\\\n",
        "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換\\\n",
        "你被給予的指示中會有明確要求要轉換成文言文還是白話文\\\n",
        "如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案\\\n",
        "如果這個要求在句尾，請直接回答：文言文答案/白話文答案\\\n",
        "以下是幾個範例：\\\n",
        "範例一：\\\n",
        "USER:翻譯成文言文：\\n於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。\\\n",
        "ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。\\\n",
        "範例二：\\\n",
        "USER:文言文翻譯：\\n靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 \\\n",
        "ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。\\\n",
        "範例三：\\\n",
        "USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\\n這句話在古代怎麼說： \\\n",
        "ASSISTANT:以後幸長官留意，勿令如此。\\\n",
        "現在是你被給予的指示：\\\n",
        "USER:{instruction} \\\n",
        "ASSISTANT:\\\n",
        "\""
      ],
      "metadata": {
        "id": "zT7SsmWq4eIq"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_instructions = [few_shot_prompt(item[\"instruction\"]) for item in sample_data]"
      ],
      "metadata": {
        "id": "NH2H7dVF9wsW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_instructions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "yAolQqOV90GU",
        "outputId": "f33d2ace-e739-4a80-d148-04a3d519c254"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\\n於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\\n靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\\n這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:建武元年，拜為禦史中丞。\\n把這句話翻譯成文言文： ASSISTANT:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_predictions = []\n",
        "\n",
        "for i, instruction in enumerate(tqdm(few_shot_instructions, desc=\"Processing Instructions\")):\n",
        "    # Prepare input tensor for model\n",
        "    inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output using the model\n",
        "    with torch.no_grad():\n",
        "        generated_ids = base_model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,  # Adjust max_length if needed\n",
        "            num_beams=15,     # Beam search can improve results\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(generated_text)\n",
        "    # Extract only the part after \"ASSISTANT:\" in the generated output\n",
        "    assistant_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "    print(assistant_text)\n",
        "\n",
        "    # Append the result to predictions list\n",
        "    few_shot_predictions.append({\n",
        "        \"id\": private_test_data[i][\"id\"],\n",
        "        \"output\": assistant_text\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2H-0KBwUAj",
        "outputId": "ec6aed95-60fb-440b-c3f4-5ed811cf1617"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Instructions:  10%|█         | 1/10 [00:01<00:15,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:建武元年，拜為禦史中丞。\n",
            "把這句話翻譯成文言文： ASSISTANT:建武元年，拜禦史中丞。\n",
            "建武元年，拜禦史中丞。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  20%|██        | 2/10 [00:03<00:11,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:文言文翻譯：\n",
            "上嘉納之。 ASSISTANT:答案：上嘉納之。\n",
            "答案：上嘉納之。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  30%|███       | 3/10 [00:05<00:13,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:翻譯成文言文：\n",
            "親自耕作的原因，就是錶示重視祭祀的供品。\n",
            "答案： ASSISTANT:親自耕作，以示重視祭祀的供品。\n",
            "親自耕作，以示重視祭祀的供品。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  40%|████      | 4/10 [00:08<00:13,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:翻譯成現代文：\n",
            "四十五年，五大夫賁攻韓，取十城。 ASSISTANT:答案：四十五年，五大夫賁攻韓，取十城。\n",
            "答案：四十五年，五大夫賁攻韓，取十城。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  50%|█████     | 5/10 [00:11<00:13,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:陳後主說： 那就把毛喜安置在一個小郡中，不許他再在朝廷參預政事。\n",
            "這句話在中國古代怎麼說： ASSISTANT:陳後主曰： 遂將毛喜安置於小郡，不許參政。\n",
            "陳後主曰： 遂將毛喜安置於小郡，不許參政。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  60%|██████    | 6/10 [00:14<00:11,  2.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:此一難忍也。\n",
            "翻譯成白話文： ASSISTANT:希望您以後留意，不要再齣這樣的事，您的小女兒病就會好。\n",
            "希望您以後留意，不要再齣這樣的事，您的小女兒病就會好。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  70%|███████   | 7/10 [00:24<00:15,  5.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:到崇政殿受冊就夠瞭！ 元宵節燈宴，太後的母親應當入宮，太後製止說 ：夫人登樓，皇上一定增加禮儀，這樣由於我的原因而超越典章製度，於心很不安啊！ 隻令賜給她燈燭，於是每年以此為常例。\n",
            "這句話在古代怎麼說： ASSISTANT:元宵節燈宴，太後的母親應當入宮，太後製止說 ：夫人登樓，皇上一定增加禮儀，這樣由於我的原因而超越典章製度，於心很不安啊！ 隻令賜給她燈燭，於是每年以此為常例。\n",
            "元宵節燈宴，太後的母親應當入宮，太後製止說 ：夫人登樓，皇上一定增加禮儀，這樣由於我的原因而超越典章製度，於心很不安啊！ 隻令賜給她燈燭，於是每年以此為常例。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  80%|████████  | 8/10 [00:26<00:08,  4.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:以己之心為根據來要求彆人，把自己的好惡也用到彆人身上。\n",
            "幫我把這句話翻譯成文言文 ASSISTANT:以己心為之，以己惡為之。\n",
            "以己心為之，以己惡為之。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Instructions:  90%|█████████ | 9/10 [00:28<00:03,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:人或鑿崖取之，即風雷為變。\n",
            "翻譯成白話文： ASSISTANT:人或鑿崖取之，即風雷為變。\n",
            "人或鑿崖取之，即風雷為變。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Instructions: 100%|██████████| 10/10 [00:29<00:00,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請你扮演一個人工智慧國文助理，幫助用戶在白話文和文言文之間轉換你被給予的指示中會有明確要求要轉換成文言文還是白話文如果這個要求在句首，請用這種格式回答：答案：文言文答案/白話文答案如果這個要求在句尾，請直接回答：文言文答案/白話文答案以下是幾個範例：範例一：USER:翻譯成文言文：\n",
            "於是，廢帝讓瀋慶之的堂侄、直將軍瀋攸之賜瀋慶之毒藥，命瀋慶之自殺。ASSISTANT:帝乃使慶之從父兄子直閣將軍攸之賜慶之藥。範例二：USER:文言文翻譯：\n",
            "靈鑒忽臨，忻歡交集，乃迴燈拂席以延之。 ASSISTANT:答案：靈仙忽然光臨，趙旭歡欣交集，於是他就把燈點亮，拂拭乾淨床席來延請仙女。範例三：USER:希望您以後留意，不要再齣這樣的事，你的小女兒病就會好。\n",
            "這句話在古代怎麼說： ASSISTANT:以後幸長官留意，勿令如此。現在是你被給予的指示：USER:翻譯成文言文：\n",
            "她高壽多少？\n",
            "答案： ASSISTANT:高壽多少？\n",
            "高壽多少？\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to a JSON file\n",
        "with open(\"few_shot_prediction.json\", \"w\") as f:\n",
        "    json.dump(few_shot_predictions, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Inference completed and predictions saved to few_shot_prediction.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCwBpGuR-LfG",
        "outputId": "d91a8ff4-3072-4f3b-c47b-1a6ce1eb3f08"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed and predictions saved to few_shot_prediction.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqfbFf2B-Xd4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}